{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据伦理\n",
    "\n",
    "### 边栏：鸣谢：Rachel Thomas博士\n",
    "\n",
    "本章由fast.ai的联合创始人、旧金山大学应用数据伦理中心的创始主任Rachel Thomas博士合著。它在很大程度上遵循了她为数据伦理导论课程制定的教学大纲的一个子集。\n",
    "\n",
    "### 结束边栏"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们在第一章和第二章中所讨论的，有时机器学习模型可能会出错。它们可能有bug。他们可能看到以前从未见过的数据，并以我们意想不到的方式行事。或者它们可以完全按照设计工作，但也可以用于我们希望它们不会被使用到的地方。\n",
    "\n",
    "因为深度学习是一种强大的工具，可以用于很多事情，所以我们考虑自己选择的后果变得尤为重要。*伦理学*的哲学研究是对是非的研究，包括我们如何定义这些术语，承认是非行为，以及理解行为与后果之间的联系。*数据伦理*领域已经存在很长一段时间了，有许多学者专注于这一领域。它被用来帮助确定许多司法管辖区的政策；大大小小的公司都在使用它来考虑如何最好地确保产品开发带来良好的社会成果；研究人员正在使用它，他们希望确保他们所做的工作是有益的，而不是有害的。\n",
    "\n",
    "因此，作为一名深度学习从业者，在某个时候，你很可能会陷入需要考虑数据伦理的境地。那么什么是数据伦理呢？这是伦理学的一个子领域，所以让我们从这里开始。\n",
    "\n",
    "> J： 在大学里，伦理哲学是我的主要内容（如果我完成了它，它将成为我论文的主题，而不是辍学加入现实世界）。根据我研究伦理学的这些年，我可以告诉你：什么是对的和错的，它们是否存在，如何发现它们，哪些人是好的，哪些人坏的，或者几乎其他任何事情，没有人真正同意。所以不要对这个理论抱有太多期望！我们将在这里重点讨论例子和思想起点，而不是理论。\n",
    "\n",
    "Markkula应用伦理学中心在回答[“什么是伦理学”](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/)的问题时表示，这个词指的是：\n",
    "\n",
    "- 有根据的是非标准，规定了人类应该做什么\n",
    "- 道德标准的研究和发展。\n",
    "\n",
    "这里没有列出正确的答案。没有做和不做的清单。伦理是复杂的，并且依赖于上下文。它涉及许多利益相关者的观点。道德是你必须培养和锻炼的一块肌肉。在本章中，我们的目标是提供一些路标来帮助你踏上这段旅程。\n",
    "\n",
    "作为合作团队的一部分，发现道德问题是最好的。这是你能够真正融入不同观点的唯一方法。不同的人的背景会帮助他们看到对你来说可能不明显的事情。与团队合作有助于许多“肌肉锻炼”活动，包括这项活动。\n",
    "\n",
    "这一章当然不是本书中我们谈论数据伦理的唯一部分，但有一个地方让我们关注它一段时间是很好的。为了确定方向，看几个例子可能是最简单的。因此，我们挑选了三个我们认为能有效说明一些关键主题的案例。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据伦理的关键示例\n",
    "\n",
    "我们将从三个具体的例子开始，这些例子说明了技术中的三个常见道德问题：\n",
    "1. *追踪流程*————阿肯色州糟糕的医疗算法让患者陷入困境。\n",
    "2. *反馈循环*————YouTube的推荐系统引发了阴谋论热潮。\n",
    "3. *偏差*————当在谷歌上搜索一个传统的非裔美国人的名字时，它会显示犯罪背景调查的广告。\n",
    "\n",
    "事实上，对于我们在本章中介绍的每一个概念，我们都将提供至少一个具体的例子。对于每一件事，想想在这种情况下你可以做什么，以及在完成这件事时可能会遇到什么样的障碍。你会如何处理他们？你会注意什么？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 漏洞和追踪：用于医疗福利的漏洞算法\n",
    "Verge调查了美国一半以上州使用的软件，以确定人们获得了多少医疗保健，并在[\"What Happens When an Algorithm Cuts Your Healthcare\"](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy)一文中记录了他们的发现。在阿肯色州实施该算法后，数百人（其中许多人患有严重残疾）的医疗保健大幅削减。例如，患有脑瘫的妇女塔米·多布斯（Tammy Dobbs）需要援助她起床、上厕所、获取食物等，她的援助时间突然减少了每周20小时。她无法解释为什么她的医疗保健被削减。最终，一个法庭案件显示，算法的软件实现存在错误，对糖尿病或脑瘫患者产生负面影响。然而，多布斯和许多其他依赖这些医疗福利的人生活在恐惧中，担心他们的福利可能会再次突然莫名其妙地被削减。\n",
    "\n",
    "### 反馈循环：YouTube的推荐系统\n",
    "当你的模型控制你获得的下一轮数据时，可能会出现反馈循环。返回的数据很快就会因软件本身而出现缺陷。\n",
    "\n",
    "例如，YouTube有19亿用户，他们每天观看超过10亿小时的YouTube视频。它的推荐算法（由谷歌构建）旨在优化观看时间，负责大约70%的观看内容。但有一个问题：它导致了失控的反馈循环，导致《纽约时报》以“YouTube掀起了阴谋论热潮。它能被遏制吗？”为标题。表面上看，推荐系统是在预测人们会喜欢什么内容，但它们在决定人们看到什么内容方面也有很大的影响力。\n",
    "\n",
    "### 偏差：Latanya Sweeney教授“被捕”\n",
    "Latanya Sweeney博士是哈佛大学的教授，也是该大学数据隐私实验室的主任。在[\"Discrimination in Online Ad Delivery\"](https://arxiv.org/abs/1301.6822)一文中（见<Latanya_arrested>>），她描述了自己的发现，即在谷歌上搜索她的名字会导致广告上写着“Latanya Swieney，被捕了？”尽管她是已知唯一的Latanya Sweeney，并且从未被捕。然而，当她在谷歌上搜索其他名字时，比如“Kirsten Lindquist”，她得到了更中立的广告，尽管Kirsten Lindquist已经被捕三次。\n",
    "\n",
    "![](../images/03.ethics/image1.png)\n",
    "\n",
    "作为一名计算机科学家，她系统地研究了这一点，并研究了2000多个名字。她发现了一个明显的模式，即历史上黑人的名字收到的广告表明此人有犯罪记录，而白人的名字则有更中立的广告。\n",
    "\n",
    "这是一个偏差的例子。这会对人们的生活产生很大的影响，例如，如果求职者在谷歌上被搜索，他们可能会有犯罪记录，而事实并非如此。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么这很重要？\n",
    "考虑到这些问题，一个非常自然的反应是：“那又怎样？这和我有什么关系？我是一名数据科学家，而不是政治家。我不是公司的高管之一，他们会决定我们做什么。我只是在尽我所能建立最具预测性的模型。”\n",
    "\n",
    "这些都是非常合理的问题。但我们将试图说服你，答案是，每个训练模型的人都绝对需要考虑如何使用他们的模型，并考虑如何最好地确保它们被尽可能积极地使用。有些事情你可以做。如果你不做，事情可能会很糟糕。\n",
    "\n",
    "IBM和纳粹德国的故事是一个特别可怕的例子，说明了当技术人员不惜一切代价专注于技术时会发生什么。2001年，一名瑞士法官裁定，“可以推断，IBM的技术援助为纳粹犯下反人类罪提供了便利，这些行为还涉及IBM机器的会计和分类，并在集中营中使用。”\n",
    "\n",
    "你看，IBM为纳粹提供了必要的数据制表产品，以跟踪大规模灭绝犹太人和其他群体的情况。这是由公司高层推动的，向希特勒及其领导团队进行营销。1939年，公司总裁托马斯·沃森亲自批准发布IBM特殊的字母排序机器，以帮助组织驱逐波兰犹太人。图为阿道夫·希特勒（最左）与IBM首席执行官老汤姆·沃森（左二）会面，就在1937年希特勒授予沃森“为帝国服务”特别奖章之前不久。\n",
    "\n",
    "![](../images/03.ethics/image2.png)\n",
    "\n",
    "但这并不是一个孤立的事件，该组织的参与是广泛的。IBM及其子公司在集中营现场提供定期培训和维护：打印卡片、配置机器，并在机器频繁损坏时进行维修。IBM在其打卡系统上对每个人被杀的方式、他们被分配到哪个小组以及在庞大的屠杀系统中追踪他们所需的后勤信息进行了分类。IBM对集中营中犹太人的编码是8：大约有6000000人被杀。它对吉普赛人的代号是12（他们被纳粹标记为“非犹太人”，有30多万人在*Zigeunerrager*或“吉普赛营地”被杀）。一般处决被编码为4，毒气室中的死亡被编码为6。\n",
    "\n",
    "![](../images/03.ethics/image3.jpeg)\n",
    "\n",
    "当然，参与其中的项目经理、工程师和技术人员只是过着普通的生活。照顾家人，周日去教堂，尽最大努力做好自己的工作。听从命令。营销人员只是在尽他们所能来实现他们的业务发展目标。正如《IBM与大屠杀》（Dialog Press）一书的作者埃德温·布莱克（Edwin Black）所观察到的那样：“对于盲目的技术官僚来说，手段比目的更重要。对犹太人民的毁灭变得更加不重要，因为在世界各地都排满了面包线的时候，IBM技术成就的振奋人心的本质只会因为获得的巨额利润而更加突出。”\n",
    "\n",
    "退后一步，思考一下：如果你发现自己是最终伤害社会的系统的一部分，你会有什么感觉？你愿意知道吗？你如何才能确保这种情况不会发生？我们在这里描述了最极端的情况，但今天观察到许多与人工智能和机器学习有关的负面社会后果，其中一些我们将在本章中描述。\n",
    "\n",
    "这也不仅仅是一种道德负担。有时技术人员会直接为他们的行为买单。例如，第一个因大众汽车丑闻而入狱的人，不是监督该项目的经理，也不是公司的高管，正是其中一位工程师James Liang，按照指令行事。大众汽车丑闻揭露了该汽车公司在柴油排放测试中作弊的行为。\n",
    "\n",
    "当然，这也不全是坏事，如果你参与的一个项目对哪怕任何一个人都产生了巨大的积极影响，这会让你感觉很好！\n",
    "\n",
    "好吧，希望我们已经说服你，你应该关心。但你该怎么办？作为数据科学家，我们自然倾向于通过优化某些指标或其他指标来改进我们的模型。但优化这一指标实际上可能不会带来更好的结果。即使它确实有助于创造更好的结果，但几乎可以肯定的是，它不会是唯一重要的事情。考虑研究人员或从业者开发模型或算法之间的步骤管道，以及这项工作实际用于做出某些决策的点。如果我们想获得我们想要的结果，就需要将整个管道作为一个整体来考虑。\n",
    "\n",
    "通常情况下，从一端到另一端有一条很长的链条。如果你是一个研究人员，这一点尤其真实，你可能甚至不知道你的研究是否会被用于任何事情，或者如果你参与了数据收集，那就更早了。但是，没有人比你更适合向这个链中的每个人介绍你的能力、限制和工作细节。尽管没有“灵丹妙药”可以确保你的工作以正确的方式使用，但通过参与过程并提出正确的问题，你至少可以确保正确的问题得到考虑。\n",
    "\n",
    "有时，对被要求做一项工作的正确反应是直接说 \"不\"。然而，我们经常听到的回答是：\"如果我不做，别人会做。\" 但是，请考虑一下：如果你被选中做这项工作，你就是他们找到的最好的人选，所以如果你不做，最好的人就没有在这个项目上工作。如果他们问的前五个人也都说不，那就更好了！\"！\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将机器学习与产品设计相结合\n",
    "\n",
    "据推测，你做这项工作的原因是你希望它能被用来做些什么。否则，你只是在浪费你的时间。所以，让我们假设你的工作最终会有用武之地开始。现在，在你收集数据和开发模型时，你正在做很多决定。你将把你的数据存储在哪个级别的聚合？你应该使用什么损失函数？你应该使用什么验证和训练集？你应该关注实施的简单性、推理的速度，还是模型的准确性？你的模型将如何处理域外的数据项目？它可以被微调，还是必须随着时间的推移从头开始重新训练？\n",
    "\n",
    "这些不仅仅是算法的问题。它们是数据产品的设计问题。但是，产品经理、高管、法官、记者、医生......无论谁最终开发和使用你的模型所构成的系统，都不会很好地理解你所做的决定，更不用说改变它们。\n",
    "\n",
    "例如，两项研究发现，亚马逊的面部识别软件产生了不准确和有种族偏差的结果。亚马逊声称，研究人员应该改变默认参数，但没有解释这将如何改变有偏差的结果。此外，事实证明，亚马逊也没有指示使用其软件的警察部门这样做。据推测，开发这些算法的研究人员和编写提供给警察的指南的亚马逊员工之间存在很大的距离。缺乏紧密的整合导致整个社会、警察和亚马逊本身出现严重问题。事实证明，他们的系统错误地将28名国会议员与犯罪分子的照片相匹配！（从图中可以看出，与罪犯照片错误匹配的国会议员中，有色人种的比例过高。）。\n",
    "\n",
    "![](../images/03.ethics/image4.png)\n",
    "\n",
    "数据科学家需要成为一个跨学科团队的一部分。研究人员需要与最终使用其研究的各类人员密切合作。更好的是，如果领域专家本身已经学到了足够的知识，能够自己训练和调试一些模型————希望你们中的一些人现在正在阅读这本书!\n",
    "\n",
    "现代工作环境是一个非常专业化的区域。每个人往往都有明确界定的工作要做。特别是在大公司，可能很难知道所有的拼图碎片是什么。如果公司知道他们的员工不会喜欢这些答案，有时公司甚至故意模糊正在进行的整体项目目标。这有时是通过尽可能地将碎片分割开来完成的。\n",
    "\n",
    "换句话说，我们不是说这些都很容易。它很难。这真的很难。我们都必须尽力而为。我们经常看到，那些参与到这些项目的更高层次的背景中，并试图发展跨学科能力和团队的人，成为他们组织中最重要和最有回报的成员。这种工作往往受到高级管理人员的高度赞赏，即使有时中层管理人员认为这是相当不舒服的。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据伦理专题\n",
    "\n",
    "数据伦理是一个很大的领域，我们不可能涵盖一切。相反，我们将挑选几个我们认为特别相关的主题：\n",
    "\n",
    "- 追索和问责的需要\n",
    "- 反馈循环\n",
    "- 偏差\n",
    "- 虚假信息\n",
    "\n",
    "让我们依次看一下。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 追诉和问责\n",
    "\n",
    "在一个复杂的系统中，很容易出现没有一个人觉得自己应当对结果负责。虽然这是可以理解的，但这并不能带来好的结果。在之前阿肯色州医疗系统的例子中，一个错误导致脑瘫患者失去了获得所需援助的机会，算法的创造者指责政府官员，而政府官员则指责实施软件的人。纽约大学教授Danah Boyd描述了这种现象：\"官僚主义经常被用来转移或逃避责任...今天的算法系统正在扩展官僚主义\"。\n",
    "\n",
    "追索如此必要的另一个原因是，数据经常包含错误。审计和纠错机制是至关重要的。加利福尼亚州执法官员维护的疑似帮派成员数据库被发现有很多错误，包括42名不到1岁的婴儿被添加到数据库中（其中28人被标记为\"承认是帮派成员\"）。在这种情况下，没有纠正错误的程序，也没有在人们被添加后将其删除的程序。另一个例子是美国的信用报告系统：在联邦贸易委员会（FTC）2012年对信用报告的大规模研究中，发现26%的消费者在他们的档案中至少有一个错误，5%的消费者档案可能存在毁灭性的错误。然而，纠正这种错误的过程是令人难以置信的缓慢和不透明的。当公共电台记者鲍比-阿林（Bobby Allyn）发现他被错误地列为有枪支犯罪记录时，他花了\"十几个电话，一个县法院书记员的书信和六个星期才解决这个问题。而这只是在我以记者的身份与公司的通信部门联系之后\"。\n",
    "\n",
    "作为机器学习从业者，我们不总是认为了解我们的算法最终如何在实践中实现是我们的责任。但我们需要这样做。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反馈循环\n",
    "\n",
    "我们在<<chapter_intro>>中解释了一个算法如何与环境互动，形成一个反馈循环，做出预测，加强在现实世界中采取的行动，从而导致在同一方向上更加明显的预测。作为一个例子，让我们再次考虑YouTube的推荐系统。几年前，谷歌团队谈到了他们如何引入强化学习（与深度学习密切相关，但你的损失函数代表了一个行动发生后很长时间内可能出现的结果）来改善YouTube的推荐系统。他们描述了他们如何使用一种算法来进行推荐，使观看时间得到优化。\n",
    "\n",
    "然而，人类往往会被有争议的内容所吸引。这意味着推荐系统开始推荐越来越多的有关阴谋论等事物的视频。此外，事实证明，对阴谋论感兴趣的人也是看了很多在线视频的人！因此，YouTube吸引了越来越多的此类人！越来越多的阴谋论者在YouTube上观看视频，导致算法推荐越来越多的阴谋论和其他极端主义内容，这导致更多的极端主义者在YouTube上观看视频，更多的人在观看YouTube时产生极端主义观点，这导致算法推荐更多的极端主义内容...... 该系统正在失去控制。\n",
    "\n",
    "而这种现象并不仅包含在这种特定类型的内容中。2019年6月，《纽约时报》发表了一篇关于YouTube推荐系统的文章，题目是\"在YouTube的数字操场上，为恋童癖者敞开大门\"。文章以这个令人不寒而栗的故事开始：\n",
    "\n",
    "> :克里斯蒂安娜.当她10岁的女儿和一个朋友上传了一段他们在后院游泳池玩耍的视频时，克里斯蒂安娜没有预料到任何事情...几天后...该视频有成千上万的浏览量。没过多久，它的浏览量就上升到了40万次...\"我再次看到这段视频，被浏览量吓到了，\"克里斯蒂安娜说。她有理由这样做。一个研究小组发现，YouTube的自动推荐系统...已经开始向观看过其他未成年、部分穿衣的儿童视频的用户展示该视频。\n",
    "\n",
    "> : 就其本身而言，每段视频可能是完全无辜的，比如说，一个孩子制作的家庭电影。任何暴露的画面都是转瞬即逝的，看起来是偶然的。但是，如果把它们组合在一起，它们的共同特征就变得明确无误了。\n",
    "\n",
    "YouTube的推荐算法已经开始为恋童癖者策划播放列表，挑选出无辜的家庭视频，其中恰好有未成年的、部分穿衣的儿童。\n",
    "\n",
    "谷歌并不打算创建一个系统，将家庭视频变成恋童癖者的色情视频。那么发生了什么？\n",
    "\n",
    "这里的部分问题是指标在驱动一个财政至上系统中的核心地位。正如你所看到的，当一个算法有一个要优化的指标时，它将尽其所能地优化这个数字。这往往会导致各种边缘情况，而与系统互动的人类会寻找、发现并利用这些边缘情况和反馈循环，以获得他们的优势。\n",
    "\n",
    "有迹象表明，这正是YouTube的推荐系统所发生的情况。*卫报*刊登了一篇名为\"前YouTube内部人士如何调查其秘密算法\"的文章，介绍了创建AlgoTransparency的前YouTube工程师Guillaume Chaslot，该文章追踪了这些问题。在罗伯特-穆勒的\"俄罗斯干预2016年总统选举调查报告\"发布后，Chaslot在<<ethics_yt_rt>>中发表了图表。\n",
    "\n",
    "![](../images/03.ethics/image18.jpeg)\n",
    "\n",
    "《今日俄罗斯》对穆勒报告的报道在有多少频道推荐方面是一个极端的异类。这表明《今日俄罗斯》这个俄罗斯国有媒体有可能成功地操纵了YouTube的推荐算法。不幸的是，这样的系统缺乏透明度，使得我们很难发现我们正在讨论的各种问题。\n",
    "\n",
    "我们这本书的评论家之一Aurélien Géron在2013年至2016年（早在这里讨论的事件之前）领导了YouTube的视频分类团队。他指出，不仅仅是涉及人类的反馈循环这一个问题。在没有人的情况下也可以有反馈循环！他告诉我们YouTube上的一个例子：\n",
    "\n",
    "> ：对视频主题进行分类的一个重要信号是它来自哪个频道。例如，上传到烹饪频道的视频很可能是烹饪视频。但是我们如何知道一个频道是关于什么主题的呢？嗯...在一定程度上是通过看它所包含的视频的主题！你看到环路了吗？例如，许多视频都有一个描述，说明是用什么相机拍摄视频的。因此，这些视频中的一些可能会被归类为关于“摄影”的视频。如果一个频道有这样一个错误分类的视频，它可能会被分类为“摄影”频道，这使得该频道未来的视频更有可能被错误地归类为“摄像”。这甚至可能导致类似病毒的分类失控！打破这种反馈循环的一种方法是对具有和不具有频道信号的视频进行分类。然后，在对通道进行分类时，你只能使用在没有通道信号的情况下获得的分类。这样，反馈循环就被打破了。\n",
    "\n",
    "有一些人和组织试图解决这些问题的积极例子。Meetup的首席机器学习工程师埃文-埃斯托拉(Evan Estola)讨论了男性对科技聚会表示出比女性更多兴趣的例子。考虑性别因素可能会导致Meetup的算法向女性推荐更少的科技聚会，因此，更少的女性会发现并参加科技聚会，这可能会导致算法向女性推荐更少的科技聚会，如此反复，形成一个自我强化的反馈循环。因此，埃文和他的团队为他们的推荐算法做出了道德上的决定，即通过明确地不使用性别作为其模型的一部分来避免产生这样的反馈循环。看到一家公司不只是不假思索地优化一个指标，而是考虑其影响，这是令人鼓舞的。埃文说：\"你需要决定在你的算法中不使用哪个功能......最优化的算法也许不是投入生产的最佳算法。\"\n",
    "\n",
    "虽然Meetup选择避免这样的结果，但Facebook提供了一个允许失控的反馈循环的例子。像YouTube一样，它倾向于通过向用户介绍更多的阴谋论来激化他们的兴趣。正如研究虚假信息扩散的研究人员Renee DiResta所写的那样：\n",
    "\n",
    ">: 一旦人们加入了一个有阴谋思想的[Facebook]团体，他们就会被算法引导到其他大量的团体。加入一个反疫苗团体，你的建议将包括反转基因、化学追踪观察、平地人（是的，真的）和\"自然治愈癌症\"的团体。推荐引擎不是把用户从兔子洞里拉出来，而是把他们进一步推入。\n",
    "\n",
    "牢记这种行为可能会发生，并在自己的项目中看到反馈循环的苗头时，预测反馈环路或采取积极的行动来打破它，这一点极为重要。另一件要记住的事情是偏差，正如我们在前一章中简单讨论的那样，它可以以非常麻烦的方式与反馈循环相互作用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏差\n",
    "\n",
    "网上关于偏差的讨论往往很快就会变得相当混乱。“偏差”这个词有很多不同的含义。统计学家经常认为，当数据伦理学家在谈论偏差时，他们是在谈论偏差一词的统计学定义。但他们不是。而且他们肯定不是在谈论出现在权重和偏差中的偏差，权重和偏差是你的模型的参数!\n",
    "\n",
    "他们所谈论的是社会科学的偏差概念。在\"理解机器学习意外后果的框架 \"中，麻省理工学院的哈里尼-苏雷什和约翰-古塔格描述了机器学习中的六种偏差，他们的论文中总结了这一点。\n",
    "\n",
    "![](../images/03.ethics/pipeline_diagram.svg)\n",
    "\n",
    "我们将讨论其中四种类型的偏差，这些偏差是我们在自己的工作中发现的最有帮助的偏差（关于其他偏差的细节，请参见论文）。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
